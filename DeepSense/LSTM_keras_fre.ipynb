{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "SEPCTURAL_SAMPLES = 10\n",
    "FEATURE_DIM = SEPCTURAL_SAMPLES*6*2\n",
    "CONV_LEN = 3\n",
    "CONV_LEN_INTE = 3#4\n",
    "CONV_LEN_LAST = 3#5\n",
    "CONV_NUM = 64\n",
    "CONV_MERGE_LEN = 8\n",
    "CONV_MERGE_LEN2 = 6\n",
    "CONV_MERGE_LEN3 = 4\n",
    "CONV_NUM2 = 64\n",
    "INTER_DIM = 128\n",
    "OUT_DIM = 6#len(idDict)\n",
    "WIDE = 20\n",
    "CONV_KEEP_PROB = 0.2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_ITER_NUM = 1000\n",
    "\n",
    "select = 'a'\n",
    "\n",
    "metaDict = {'a':[119080, 1193], 'b':[116870, 1413], 'c':[116020, 1477]}\n",
    "TRAIN_SIZE = metaDict[select][0]\n",
    "EVAL_DATA_SIZE = metaDict[select][1]\n",
    "EVAL_ITER_NUM = int(math.ceil(EVAL_DATA_SIZE / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254000, 120)\n",
      "(12700, 20, 120) (12700, 6)\n",
      "(12700, 20, 60, 1) (12700, 20, 60, 1)\n",
      "[0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# ###### Import training data\n",
    "# train_x = np.load('DeepSense_data/train_x.npy')\n",
    "# train_y = np.load('DeepSense_data/train_y.npy')\n",
    "# eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "# eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "\n",
    "###### Import training data\n",
    "train_x = np.load('fre_data.npy')\n",
    "print(train_x.shape)\n",
    "train_y = np.load('fre_data_label.npy')\n",
    "# eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "# eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "train_x = train_x.reshape((-1,20,120))\n",
    "\n",
    "# train_x = np.array(train_x)\n",
    "print(train_x.shape,train_y.shape)\n",
    "# print(eval_x.shape,eval_y.shape)\n",
    "\n",
    "acc,gyo = np.split(train_x,2)\n",
    "acc = acc.reshape((-1,20,60,1))\n",
    "gyo = gyo.reshape((-1,20,60,1))\n",
    "print(acc.shape,gyo.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119080, 20, 120) (119080, 6)\n",
      "(1193, 20, 120) (1193, 6)\n",
      "[0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "###### Import training data\n",
    "train_x = np.load('DeepSense_data/train_x.npy')\n",
    "train_y = np.load('DeepSense_data/train_y.npy')\n",
    "eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "\n",
    "# train_x = np.array(train_x)\n",
    "print(train_x.shape,train_y.shape)\n",
    "print(eval_x.shape,eval_y.shape)\n",
    "# train_x = np.expand_dims(train_x, axis=3)\n",
    "# acc,gyo = np.split(train_x,2,axis=2)\n",
    "# acc = acc.reshape((-1,20,60,1))\n",
    "# gyo = gyo.reshape((-1,20,60,1))\n",
    "# print(acc.shape,gyo.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(12700, 20, 120) (12700, 6) 4.992907516420385e-19 5.345805606954511\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(x_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "#test_data_count = len(eval_x)  # 2947 testing series\n",
    "n_steps = len(x_train[0])  # 128 timesteps per series\n",
    "n_input = len(x_train[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# train_x = (train_x - np.min(train_x))/np.mean(train_x)\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(train_x.shape, train_y.shape, np.mean(x_train), np.std(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape\n",
    "import keras.backend as K\n",
    "from keras.layers.recurrent import GRU,LSTM\n",
    "from keras.layers import GRUCell,Lambda\n",
    "from keras.layers import RNN\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 16:59:49.346907  9748 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0917 16:59:49.366908  9748 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0917 16:59:49.368908  9748 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0917 16:59:49.918940  9748 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0917 16:59:49.930940  9748 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 20, 120)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 64)            47360     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20, 64)            33024     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 80,774\n",
      "Trainable params: 80,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 64 # Hidden layer num of features\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0005\n",
    "lambda_loss_amount = 0.0005\n",
    "#training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "batch_size = 128\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "sensor_input = Input(shape=(20,120), name='input')\n",
    "outputs = LSTM(n_hidden,unit_forget_bias=True,return_sequences=True)(sensor_input)\n",
    "outputs = LSTM(n_hidden,unit_forget_bias=True,return_sequences=True)(outputs)\n",
    "\n",
    "def reduce_dim(x):\n",
    "    return x[:,-1:,:]\n",
    "\n",
    "outputs = Lambda(reduce_dim)(outputs)\n",
    "outputs = Activation('linear')(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "\n",
    "outputs = Dense(6, activation='softmax',kernel_regularizer=regularizers.l2(lambda_loss_amount))(outputs)\n",
    "# keras.regularizers.l2(lambda_loss_amount)\n",
    "adam = keras.optimizers.Adam(lr=learning_rate,epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model = Model(inputs=sensor_input, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "      optimizer=adam,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "#model.fit([acc_inputs,gyro_inputs], train_y, batch_size=64, epochs=10)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 16:59:54.422197  9748 deprecation.py:323] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 16:59:56.043290  9748 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95264 samples, validate on 23816 samples\n",
      "Epoch 1/20\n",
      "95264/95264 [==============================] - 241s 3ms/step - loss: 0.1629 - acc: 0.9456 - val_loss: 0.1406 - val_acc: 0.9694\n",
      "Epoch 2/20\n",
      "95264/95264 [==============================] - 234s 2ms/step - loss: 0.0285 - acc: 0.9943 - val_loss: 0.1304 - val_acc: 0.9650\n",
      "Epoch 3/20\n",
      "12800/95264 [===>..........................] - ETA: 3:04 - loss: 0.0199 - acc: 0.9973"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=64,validation_split=0.2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "2540/2540 [==============================] - 1s 368us/step\n",
      "loss, acc  0.10084378505785634 0.9724409443186963\n",
      "2540/2540 [==============================] - 1s 356us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       386\n",
      "           1       1.00      1.00      1.00       500\n",
      "           2       0.89      0.99      0.94       358\n",
      "           3       0.97      0.95      0.96       399\n",
      "           4       0.99      1.00      0.99       417\n",
      "           5       1.00      0.93      0.96       480\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2540\n",
      "   macro avg       0.97      0.97      0.97      2540\n",
      "weighted avg       0.97      0.97      0.97      2540\n",
      " samples avg       0.97      0.97      0.97      2540\n",
      "\n",
      "0.9726304060305165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "test_x = x_test\n",
    "test_y = y_test\n",
    "\n",
    "# print ( \"Loss = \" + str( preds[0] ) )\n",
    "# print ( \"Test Accuracy = \" + str( preds[1] ) )\n",
    "print(model.metrics_names)\n",
    "loss, acc = model.evaluate(test_x, test_y, batch_size=128)\n",
    "print('loss, acc ',loss,acc)\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_x, batch_size=128, verbose=1)\n",
    "# y_pred_bool = np.argmax(preds, axis=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0\n",
    "\n",
    "print(classification_report(test_y, y_pred))\n",
    "print(metrics.f1_score(test_y, y_pred, average=\"weighted\"))\n",
    "# print(confusion_matrix(test_y.argmax(axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9724409448818898\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(test_y, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
