{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "SEPCTURAL_SAMPLES = 10\n",
    "FEATURE_DIM = SEPCTURAL_SAMPLES*6*2\n",
    "CONV_LEN = 3\n",
    "CONV_LEN_INTE = 3#4\n",
    "CONV_LEN_LAST = 3#5\n",
    "CONV_NUM = 64\n",
    "CONV_MERGE_LEN = 8\n",
    "CONV_MERGE_LEN2 = 6\n",
    "CONV_MERGE_LEN3 = 4\n",
    "CONV_NUM2 = 64\n",
    "INTER_DIM = 128\n",
    "OUT_DIM = 6#len(idDict)\n",
    "WIDE = 20\n",
    "CONV_KEEP_PROB = 0.2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_ITER_NUM = 1000\n",
    "\n",
    "select = 'a'\n",
    "\n",
    "metaDict = {'a':[119080, 1193], 'b':[116870, 1413], 'c':[116020, 1477]}\n",
    "TRAIN_SIZE = metaDict[select][0]\n",
    "EVAL_DATA_SIZE = metaDict[select][1]\n",
    "EVAL_ITER_NUM = int(math.ceil(EVAL_DATA_SIZE / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119080, 20, 120) (119080, 6)\n",
      "(1193, 20, 120) (1193, 6)\n",
      "[0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "###### Import training data\n",
    "train_x = np.load('DeepSense_data/train_x.npy')\n",
    "train_y = np.load('DeepSense_data/train_y.npy')\n",
    "eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "\n",
    "# train_x = np.array(train_x)\n",
    "print(train_x.shape,train_y.shape)\n",
    "print(eval_x.shape,eval_y.shape)\n",
    "# train_x = np.expand_dims(train_x, axis=3)\n",
    "# acc,gyo = np.split(train_x,2,axis=2)\n",
    "# acc = acc.reshape((-1,20,60,1))\n",
    "# gyo = gyo.reshape((-1,20,60,1))\n",
    "# print(acc.shape,gyo.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254000, 120)\n",
      "(12700, 20, 120) (12700, 6)\n",
      "(12700, 20, 60, 1) (12700, 20, 60, 1)\n",
      "[0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "###### Import training data\n",
    "train_x = np.load('fre_data.npy')\n",
    "print(train_x.shape)\n",
    "train_y = np.load('fre_data_label.npy')\n",
    "# eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "# eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "train_x = train_x.reshape((-1,20,120))\n",
    "\n",
    "# train_x = np.array(train_x)\n",
    "print(train_x.shape,train_y.shape)\n",
    "# print(eval_x.shape,eval_y.shape)\n",
    "\n",
    "acc,gyo = np.split(train_x,2)\n",
    "acc = acc.reshape((-1,20,60,1))\n",
    "gyo = gyo.reshape((-1,20,60,1))\n",
    "print(acc.shape,gyo.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(119080, 20, 120, 1) (119080, 6) 0.44252616 9.060849\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(train_x)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(eval_x)  # 2947 testing series\n",
    "n_steps = len(train_x[0])  # 128 timesteps per series\n",
    "n_input = len(train_x[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# train_x = (train_x - np.min(train_x))/np.mean(train_x)\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(train_x.shape, train_y.shape, np.mean(train_x), np.std(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape\n",
    "import keras.backend as K\n",
    "from keras.layers.recurrent import GRU,LSTM\n",
    "from keras.layers import GRUCell,Lambda\n",
    "from keras.layers import RNN\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 17:09:45.217334 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0917 17:09:45.242334 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0917 17:09:45.244334 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0917 17:09:45.281336 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0917 17:09:45.284336 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0917 17:09:47.001401 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0917 17:09:47.100404 10600 deprecation.py:506] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_conv3_shape [None, 20, 14, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 17:09:48.582455 10600 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 20, 120, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 18, 64)        1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 18, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 16, 64)        12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 14, 64)        12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 20, 896)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20, 128)           492032    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 128)           98816     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 15366     \n",
      "=================================================================\n",
      "Total params: 632,902\n",
      "Trainable params: 632,518\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 64 # Hidden layer num of features\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.001\n",
    "lambda_loss_amount = 0.001\n",
    "#training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
    "batch_size = 64\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "sensor_input = Input(shape=(20,120,1), name='input')\n",
    "\n",
    "acc_conv1 = Conv2D(CONV_NUM, kernel_size=[1, 2*3*CONV_LEN],\n",
    "        strides=(1, 2*3), padding='VALID', activation=None, data_format='channels_last')(sensor_input)\n",
    "acc_conv1 = BatchNormalization()(acc_conv1)\n",
    "acc_conv1 = Activation('relu')(acc_conv1)\n",
    "acc_conv1_shape = acc_conv1.get_shape().as_list()\n",
    "acc_conv1 = Dropout(CONV_KEEP_PROB, noise_shape=[acc_conv1_shape[0], 1, 1, acc_conv1_shape[3]], seed=None)(acc_conv1)\n",
    "#         layers.dropout(acc_conv1, CONV_KEEP_PROB, is_training=train,\n",
    "#             noise_shape=[acc_conv1_shape[0], 1, 1, acc_conv1_shape[3]], scope='acc_dropout1')\n",
    "\n",
    "acc_conv2 = Conv2D(CONV_NUM, kernel_size=[1, CONV_LEN_INTE],\n",
    "        strides=(1, 1), padding='VALID', activation=None, data_format='channels_last')(acc_conv1)\n",
    "acc_conv2 = BatchNormalization()(acc_conv2)\n",
    "acc_conv2 = Activation('relu')(acc_conv2)\n",
    "acc_conv2_shape = acc_conv2.get_shape().as_list()\n",
    "acc_conv2 = Dropout(CONV_KEEP_PROB, noise_shape=[acc_conv2_shape[0], 1, 1, acc_conv2_shape[3]], seed=None)(acc_conv2)\n",
    "#layers.dropout(acc_conv2, CONV_KEEP_PROB, is_training=train,\n",
    "#             noise_shape=[acc_conv2_shape[0], 1, 1, acc_conv2_shape[3]], scope='acc_dropout2')\n",
    "\n",
    "acc_conv3 = Conv2D(CONV_NUM, kernel_size=[1, CONV_LEN_LAST],\n",
    "        strides=(1, 1), padding='VALID', activation=None, data_format='channels_last')(acc_conv2)\n",
    "acc_conv3 = BatchNormalization()(acc_conv3)\n",
    "acc_conv3 = Activation('relu')(acc_conv3)\n",
    "acc_conv3_shape = acc_conv3.get_shape().as_list()\n",
    "print('acc_conv3_shape',acc_conv3_shape)\n",
    "outputs = Reshape((acc_conv3_shape[1], -1))(acc_conv3)\n",
    "\n",
    "outputs = Bidirectional(LSTM(n_hidden,unit_forget_bias=True,return_sequences=True))(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Bidirectional(LSTM(n_hidden,unit_forget_bias=True,return_sequences=True))(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "\n",
    "# def reduce_dim(x):\n",
    "#     return x[:,-1:,:]\n",
    "\n",
    "# outputs = Lambda(reduce_dim)(outputs)\n",
    "outputs = Activation('linear')(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "\n",
    "outputs = Dense(6, activation='softmax',kernel_regularizer=regularizers.l2(lambda_loss_amount))(outputs)\n",
    "# keras.regularizers.l2(lambda_loss_amount)\n",
    "adam = keras.optimizers.Adam(lr=learning_rate,epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model = Model(inputs=sensor_input, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "      optimizer=adam,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "#model.fit([acc_inputs,gyro_inputs], train_y, batch_size=64, epochs=10)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 17:10:11.886041 10600 deprecation.py:323] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95264 samples, validate on 23816 samples\n",
      "Epoch 1/50\n",
      "95264/95264 [==============================] - 624s 7ms/step - loss: 0.1704 - acc: 0.9414 - val_loss: 0.1369 - val_acc: 0.9654\n",
      "Epoch 2/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0507 - acc: 0.9850 - val_loss: 0.0923 - val_acc: 0.9792\n",
      "Epoch 3/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0354 - acc: 0.9900 - val_loss: 0.0852 - val_acc: 0.9811\n",
      "Epoch 4/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0273 - acc: 0.9928 - val_loss: 0.0762 - val_acc: 0.9814\n",
      "Epoch 5/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0231 - acc: 0.9945 - val_loss: 0.0755 - val_acc: 0.9821\n",
      "Epoch 6/50\n",
      "95264/95264 [==============================] - 611s 6ms/step - loss: 0.0194 - acc: 0.9957 - val_loss: 0.0577 - val_acc: 0.9877\n",
      "Epoch 7/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0181 - acc: 0.9960 - val_loss: 0.0655 - val_acc: 0.9851\n",
      "Epoch 8/50\n",
      "95264/95264 [==============================] - 611s 6ms/step - loss: 0.0145 - acc: 0.9968 - val_loss: 0.0702 - val_acc: 0.9858\n",
      "Epoch 9/50\n",
      "95264/95264 [==============================] - 611s 6ms/step - loss: 0.0126 - acc: 0.9973 - val_loss: 0.0668 - val_acc: 0.9890\n",
      "Epoch 10/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0710 - val_acc: 0.9842\n",
      "Epoch 11/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0103 - acc: 0.9980 - val_loss: 0.0986 - val_acc: 0.9807\n",
      "Epoch 12/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0112 - acc: 0.9978 - val_loss: 0.0657 - val_acc: 0.9862\n",
      "Epoch 13/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0108 - acc: 0.9979 - val_loss: 0.0787 - val_acc: 0.9864\n",
      "Epoch 14/50\n",
      "95264/95264 [==============================] - 614s 6ms/step - loss: 0.0102 - acc: 0.9980 - val_loss: 0.0626 - val_acc: 0.9904\n",
      "Epoch 15/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0095 - acc: 0.9983 - val_loss: 0.0595 - val_acc: 0.9885\n",
      "Epoch 16/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0077 - acc: 0.9988 - val_loss: 0.0599 - val_acc: 0.9932\n",
      "Epoch 17/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0089 - acc: 0.9984 - val_loss: 0.0448 - val_acc: 0.9950\n",
      "Epoch 18/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0078 - acc: 0.9987 - val_loss: 0.0448 - val_acc: 0.9939\n",
      "Epoch 19/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0085 - acc: 0.9986 - val_loss: 0.0579 - val_acc: 0.9921\n",
      "Epoch 20/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0064 - acc: 0.9990 - val_loss: 0.0399 - val_acc: 0.9947\n",
      "Epoch 21/50\n",
      "95264/95264 [==============================] - 610s 6ms/step - loss: 0.0082 - acc: 0.9987 - val_loss: 0.0447 - val_acc: 0.9910\n",
      "Epoch 22/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0073 - acc: 0.9988 - val_loss: 0.0506 - val_acc: 0.9933\n",
      "Epoch 23/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0405 - val_acc: 0.9941\n",
      "Epoch 24/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0054 - acc: 0.9992 - val_loss: 0.0578 - val_acc: 0.9927\n",
      "Epoch 25/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0074 - acc: 0.9989 - val_loss: 0.0501 - val_acc: 0.9937\n",
      "Epoch 26/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.0429 - val_acc: 0.9929\n",
      "Epoch 27/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0056 - acc: 0.9991 - val_loss: 0.0607 - val_acc: 0.9886\n",
      "Epoch 28/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0070 - acc: 0.9990 - val_loss: 0.0462 - val_acc: 0.9950\n",
      "Epoch 29/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0064 - acc: 0.9990 - val_loss: 0.0521 - val_acc: 0.9916\n",
      "Epoch 30/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0061 - acc: 0.9991 - val_loss: 0.0539 - val_acc: 0.9934\n",
      "Epoch 31/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0059 - acc: 0.9993 - val_loss: 0.0540 - val_acc: 0.9925\n",
      "Epoch 32/50\n",
      "95264/95264 [==============================] - 609s 6ms/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0629 - val_acc: 0.9913\n",
      "Epoch 33/50\n",
      "95264/95264 [==============================] - 611s 6ms/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0595 - val_acc: 0.9849\n",
      "Epoch 34/50\n",
      "95264/95264 [==============================] - 611s 6ms/step - loss: 0.0052 - acc: 0.9993 - val_loss: 0.0566 - val_acc: 0.9898\n",
      "Epoch 35/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.0847 - val_acc: 0.9898\n",
      "Epoch 36/50\n",
      "95264/95264 [==============================] - 606s 6ms/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.0460 - val_acc: 0.9949\n",
      "Epoch 37/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.0449 - val_acc: 0.9920\n",
      "Epoch 38/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.0497 - val_acc: 0.9946\n",
      "Epoch 39/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0052 - acc: 0.9993 - val_loss: 0.0644 - val_acc: 0.9912\n",
      "Epoch 40/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0056 - acc: 0.9992 - val_loss: 0.0440 - val_acc: 0.9927\n",
      "Epoch 41/50\n",
      "95264/95264 [==============================] - 605s 6ms/step - loss: 0.0038 - acc: 0.9996 - val_loss: 0.0535 - val_acc: 0.9939\n",
      "Epoch 42/50\n",
      "95264/95264 [==============================] - 606s 6ms/step - loss: 0.0053 - acc: 0.9993 - val_loss: 0.0568 - val_acc: 0.9939\n",
      "Epoch 43/50\n",
      "95264/95264 [==============================] - 611s 6ms/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.0406 - val_acc: 0.9953\n",
      "Epoch 44/50\n",
      "95264/95264 [==============================] - 605s 6ms/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0579 - val_acc: 0.9929\n",
      "Epoch 45/50\n",
      "95264/95264 [==============================] - 606s 6ms/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0440 - val_acc: 0.9947\n",
      "Epoch 46/50\n",
      "95264/95264 [==============================] - 608s 6ms/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.0677 - val_acc: 0.9901\n",
      "Epoch 47/50\n",
      "95264/95264 [==============================] - 606s 6ms/step - loss: 0.0035 - acc: 0.9996 - val_loss: 0.0436 - val_acc: 0.9944\n",
      "Epoch 48/50\n",
      "95264/95264 [==============================] - 607s 6ms/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.0466 - val_acc: 0.9947\n",
      "Epoch 49/50\n",
      "95264/95264 [==============================] - 605s 6ms/step - loss: 0.0041 - acc: 0.9995 - val_loss: 0.0508 - val_acc: 0.9940\n",
      "Epoch 50/50\n",
      "95264/95264 [==============================] - 605s 6ms/step - loss: 0.0043 - acc: 0.9995 - val_loss: 0.0542 - val_acc: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c52c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.reshape((-1,20,120,1))\n",
    "model.fit(train_x, train_y, batch_size=64,validation_split=0.2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    " \n",
    "model.save('CNN_LSTM.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    " \n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('DeepSense_keras.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1193/1193 [==============================] - 2s 2ms/step\n",
      "loss, acc  0.5857870359432767 0.9002514659908771\n",
      "1193/1193 [==============================] - 3s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95       118\n",
      "           1       1.00      0.25      0.40         4\n",
      "           2       0.96      1.00      0.98       231\n",
      "           3       0.91      0.86      0.89       111\n",
      "           4       0.87      0.86      0.86       360\n",
      "           5       0.87      0.88      0.88       369\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1193\n",
      "   macro avg       0.93      0.80      0.83      1193\n",
      "weighted avg       0.90      0.90      0.90      1193\n",
      " samples avg       0.90      0.90      0.90      1193\n",
      "\n",
      "0.8994257682149092\n",
      "[[111   0   7   0   0   0]\n",
      " [  1   1   2   0   0   0]\n",
      " [  1   0 230   0   0   0]\n",
      " [  0   0   0  96   6   9]\n",
      " [  2   0   0   7 310  41]\n",
      " [  0   0   0   2  41 326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "test_x = eval_x.reshape((-1,20,120,1))\n",
    "test_y = eval_y\n",
    "\n",
    "# print ( \"Loss = \" + str( preds[0] ) )\n",
    "# print ( \"Test Accuracy = \" + str( preds[1] ) )\n",
    "print(model.metrics_names)\n",
    "loss, acc = model.evaluate(test_x, test_y, batch_size=64)\n",
    "print('loss, acc ',loss,acc)\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_x, batch_size=64, verbose=1)\n",
    "# y_pred_bool = np.argmax(preds, axis=1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0\n",
    "\n",
    "print(classification_report(test_y, y_pred))\n",
    "print(metrics.f1_score(test_y, y_pred, average=\"weighted\"))\n",
    "print(confusion_matrix(test_y.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
