{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "SEPCTURAL_SAMPLES = 10\n",
    "FEATURE_DIM = SEPCTURAL_SAMPLES*6*2  #120\n",
    "CONV_LEN = 3\n",
    "CONV_LEN_INTE = 3#4\n",
    "CONV_LEN_LAST = 3#5\n",
    "CONV_NUM = 64\n",
    "CONV_MERGE_LEN = 3  # original 8\n",
    "CONV_MERGE_LEN2 = 3  # original 6\n",
    "CONV_MERGE_LEN3 = 4\n",
    "CONV_NUM2 = 64\n",
    "INTER_DIM = 120\n",
    "OUT_DIM = 6#len(idDict)\n",
    "WIDE = 20\n",
    "CONV_KEEP_PROB = 0.5\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TOTAL_ITER_NUM = 1000\n",
    "\n",
    "select = 'a'\n",
    "\n",
    "metaDict = {'a':[119080, 1193], 'b':[116870, 1413], 'c':[116020, 1477]}\n",
    "TRAIN_SIZE = metaDict[select][0]\n",
    "EVAL_DATA_SIZE = metaDict[select][1]\n",
    "EVAL_ITER_NUM = int(math.ceil(EVAL_DATA_SIZE / BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape,train_y.shape  (119080, 20, 120) (119080, 6)\n",
      "eval_x.shape,eval_y.shape  (1193, 20, 120) (1193, 6)\n",
      "train_x.shape,train_y.shape  (119080, 20, 120, 1) (119080, 6)\n",
      "acc.shape,gyo.shape  (119080, 20, 60, 1) (119080, 20, 60, 1)\n",
      "[0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "###### Import training data\n",
    "train_x = np.load('DeepSense_data/train_x.npy')\n",
    "train_y = np.load('DeepSense_data/train_y.npy')\n",
    "eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "\n",
    "# train_x = np.array(train_x)\n",
    "print('train_x.shape,train_y.shape ',train_x.shape,train_y.shape)\n",
    "print('eval_x.shape,eval_y.shape ',eval_x.shape,eval_y.shape)\n",
    "train_x = np.expand_dims(train_x, axis=3)\n",
    "print('train_x.shape,train_y.shape ',train_x.shape,train_y.shape)\n",
    "acc,gyo = np.split(train_x,2,axis=2)\n",
    "print('acc.shape,gyo.shape ',acc.shape,gyo.shape)\n",
    "# acc = acc.reshape((-1,20,60,1))\n",
    "# gyo = gyo.reshape((-1,20,60,1))\n",
    "# print(acc.shape,gyo.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254000, 120)\n",
      "(12700, 20, 120, 1) (12700, 6)\n",
      "[0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "###### Import training data\n",
    "train_x = np.load('fre_data.npy')\n",
    "print(train_x.shape)\n",
    "train_y = np.load('fre_data_label.npy')\n",
    "# eval_x = np.load('DeepSense_data/eval_x.npy')\n",
    "# eval_y = np.load('DeepSense_data/eval_y.npy')\n",
    "train_x = train_x.reshape((-1,20,120))\n",
    "train_x = np.expand_dims(train_x, axis=3)\n",
    "# train_x = np.array(train_x)\n",
    "print(train_x.shape,train_y.shape)\n",
    "# print(eval_x.shape,eval_y.shape)\n",
    "\n",
    "# acc,gyo = np.split(train_x,2)\n",
    "# acc = acc.reshape((-1,20,60,1))\n",
    "# gyo = gyo.reshape((-1,20,60,1))\n",
    "# print(acc.shape,gyo.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape\n",
    "import keras.backend as K\n",
    "from keras.layers.recurrent import GRU,LSTM\n",
    "from keras.layers import GRUCell,Lambda\n",
    "from keras.layers import RNN\n",
    "from keras import regularizers\n",
    "# from keras.backend import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0919 11:04:23.504770  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0919 11:04:23.522770  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0919 11:04:23.523771  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0919 11:04:23.558773  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0919 11:04:23.559773  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_inputs.shape,gyro_inputs.shape  (119080, 20, 60, 1) (119080, 20, 60, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 11:04:25.124862  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0919 11:04:25.222868  4752 deprecation.py:506] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_conv3_shape [None, 20, 4, 64]\n",
      "acc_conv_out.shape  (?, 20, 1, 4, 64)\n",
      "gyro_conv3.shape  [None, 20, 4, 64]\n",
      "gyro_conv_out.shape  (?, 20, 1, 4, 64)\n",
      "sensor_conv_in  (?, 20, 2, 4, 64)\n",
      "sensor_conv_in  Tensor(\"dropout_5/cond/Merge:0\", shape=(?, 20, 2, 4, 64), dtype=float32)\n",
      "sensor_conv3.shape  [None, 20, 2, 4, 64]\n",
      "sensor_conv_out.shape  (?, 20, 2, 4, 64)\n",
      "shape before gru  (?, 20, ?)\n",
      "gru1.shape  (?, ?, 120)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 11:04:27.193980  4752 deprecation_wrapper.py:119] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru2.shape  (?, ?, 120)\n",
      "output of GRU  (?, 120)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             (None, 20, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2 (InputLayer)             (None, 20, 60, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 20, 8, 64)    1216        input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 8, 64)    1216        input2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 8, 64)    256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 8, 64)    256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 8, 64)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 8, 64)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 8, 64)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 8, 64)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 20, 6, 64)    12352       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 20, 6, 64)    12352       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 6, 64)    256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 6, 64)    256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 6, 64)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 6, 64)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 6, 64)    0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 6, 64)    0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 20, 4, 64)    12352       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 20, 4, 64)    12352       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 4, 64)    256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 4, 64)    256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 4, 64)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 4, 64)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 20, 1, 4, 64) 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 20, 1, 4, 64) 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 2, 4, 64) 0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 2, 4, 64) 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 20, 2, 4, 64) 24640       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 2, 4, 64) 256         conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 2, 4, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 2, 4, 64) 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 20, 2, 4, 64) 24640       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 2, 4, 64) 256         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 2, 4, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 2, 4, 64) 0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 20, 2, 4, 64) 32832       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 2, 4, 64) 256         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 2, 4, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 20, 512)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 20, 120)      227880      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 120)      0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 20, 120)      86760       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 120)      0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 120)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            726         lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 451,622\n",
      "Trainable params: 450,470\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "acc_inputs, gyro_inputs = acc,gyo\n",
    "print('acc_inputs.shape,gyro_inputs.shape ',acc_inputs.shape,gyro_inputs.shape)\n",
    "\n",
    "input1 = Input(shape=(20,60,1), name='input1')\n",
    "# sensor_inputs shape (BATCH_SIZE, WIDE, FEATURE_DIM, CHANNEL=1)\n",
    "acc_conv1 = Conv2D(CONV_NUM, kernel_size=[1, 2*3*CONV_LEN],\n",
    "        strides=(1, 2*3), padding='VALID', activation=None, data_format='channels_last')(input1)\n",
    "acc_conv1 = BatchNormalization()(acc_conv1)\n",
    "acc_conv1 = Activation('relu')(acc_conv1)\n",
    "acc_conv1_shape = acc_conv1.get_shape().as_list()\n",
    "acc_conv1 = Dropout(CONV_KEEP_PROB, noise_shape=[acc_conv1_shape[0], 1, 1, acc_conv1_shape[3]], seed=None)(acc_conv1)\n",
    "#         layers.dropout(acc_conv1, CONV_KEEP_PROB, is_training=train,\n",
    "#             noise_shape=[acc_conv1_shape[0], 1, 1, acc_conv1_shape[3]], scope='acc_dropout1')\n",
    "\n",
    "acc_conv2 = Conv2D(CONV_NUM, kernel_size=[1, CONV_LEN_INTE],\n",
    "        strides=(1, 1), padding='VALID', activation=None, data_format='channels_last')(acc_conv1)\n",
    "acc_conv2 = BatchNormalization()(acc_conv2)\n",
    "acc_conv2 = Activation('relu')(acc_conv2)\n",
    "acc_conv2_shape = acc_conv2.get_shape().as_list()\n",
    "acc_conv2 = Dropout(CONV_KEEP_PROB, noise_shape=[acc_conv2_shape[0], 1, 1, acc_conv2_shape[3]], seed=None)(acc_conv2)\n",
    "#layers.dropout(acc_conv2, CONV_KEEP_PROB, is_training=train,\n",
    "#             noise_shape=[acc_conv2_shape[0], 1, 1, acc_conv2_shape[3]], scope='acc_dropout2')\n",
    "\n",
    "acc_conv3 = Conv2D(CONV_NUM, kernel_size=[1, CONV_LEN_LAST],\n",
    "        strides=(1, 1), padding='VALID', activation=None, data_format='channels_last')(acc_conv2)\n",
    "acc_conv3 = BatchNormalization()(acc_conv3)\n",
    "acc_conv3 = Activation('relu')(acc_conv3)\n",
    "acc_conv3_shape = acc_conv3.get_shape().as_list()\n",
    "print('acc_conv3_shape',acc_conv3_shape)\n",
    "acc_conv_out = Reshape((acc_conv3_shape[1], 1, acc_conv3_shape[2],acc_conv3_shape[3]))(acc_conv3)\n",
    "# tf.reshape(acc_conv3, [acc_conv3_shape[0], acc_conv3_shape[1], 1, acc_conv3_shape[2],acc_conv3_shape[3]])\n",
    "print('acc_conv_out.shape ',acc_conv_out.shape)\n",
    "\n",
    "# acc_conv_out = Reshape((-1,15*54*64))(acc_conv3)\n",
    "\n",
    "\n",
    "input2 = Input(shape=(20,60,1), name='input2')\n",
    "gyro_conv1 = Conv2D(CONV_NUM, kernel_size=[1, 2*3*CONV_LEN],\n",
    "        strides=(1, 2*3), padding='VALID', activation=None, data_format='channels_last')(input2)\n",
    "gyro_conv1 = BatchNormalization()(gyro_conv1)\n",
    "gyro_conv1 = Activation('relu')(gyro_conv1)\n",
    "gyro_conv1_shape = gyro_conv1.get_shape().as_list()\n",
    "gyro_conv1 = Dropout(CONV_KEEP_PROB, noise_shape=[gyro_conv1_shape[0], 1, 1, gyro_conv1_shape[3]], seed=None)(gyro_conv1)\n",
    "\n",
    "gyro_conv2 = Conv2D(CONV_NUM, kernel_size=[1, CONV_LEN_INTE],\n",
    "        strides=(1, 1), padding='VALID', activation=None, data_format='channels_last')(gyro_conv1)\n",
    "gyro_conv2 = BatchNormalization()(gyro_conv2)\n",
    "gyro_conv2 = Activation('relu')(gyro_conv2)\n",
    "gyro_conv2_shape = gyro_conv2.get_shape().as_list()\n",
    "gyro_conv2 = Dropout(CONV_KEEP_PROB, noise_shape=[gyro_conv2_shape[0], 1, 1, gyro_conv2_shape[3]], seed=None)(gyro_conv2)\n",
    "\n",
    "gyro_conv3 = Conv2D(CONV_NUM, kernel_size=[1, CONV_LEN_LAST],\n",
    "        strides=(1, 1), padding='VALID', activation=None, data_format='channels_last')(gyro_conv2)\n",
    "gyro_conv3 = BatchNormalization()(gyro_conv3)\n",
    "gyro_conv3 = Activation('relu')(gyro_conv3)\n",
    "gyro_conv3_shape = gyro_conv3.get_shape().as_list()\n",
    "print('gyro_conv3.shape ',gyro_conv3_shape)\n",
    "#gyro_conv_out = tf.reshape(gyro_conv3, [gyro_conv3_shape[0], gyro_conv3_shape[1], 1, gyro_conv3_shape[2], gyro_conv3_shape[3]])\n",
    "gyro_conv_out = Reshape((gyro_conv3_shape[1], 1, acc_conv3_shape[2],acc_conv3_shape[3]))(gyro_conv3)\n",
    "print('gyro_conv_out.shape ',gyro_conv_out.shape)\n",
    "\n",
    "# gyro_conv_out = Reshape((-1,15*54*64))(gyro_conv3)\n",
    "#print('gyro_conv_out.shape ',gyro_conv_out.shape)\n",
    "sensor_conv_in = concatenate([acc_conv_out, gyro_conv_out], axis=2)\n",
    "print('sensor_conv_in ',sensor_conv_in.shape)\n",
    "senor_conv_shape = sensor_conv_in.get_shape().as_list()\n",
    "sensor_conv_in = Dropout(CONV_KEEP_PROB,seed=None)(sensor_conv_in)\n",
    "print('sensor_conv_in ',sensor_conv_in)\n",
    "\n",
    "\n",
    "sensor_conv1 = Conv3D(CONV_NUM2, kernel_size=[1, 2, CONV_MERGE_LEN],\n",
    "                strides=(1, 1, 1), padding='SAME', activation=None, data_format='channels_last')(sensor_conv_in)\n",
    "sensor_conv1 = BatchNormalization()(sensor_conv1)\n",
    "sensor_conv1 = Activation('relu')(sensor_conv1)\n",
    "sensor_conv1_shape = sensor_conv1.get_shape().as_list()\n",
    "sensor_conv1 = Dropout(CONV_KEEP_PROB,seed=None)(sensor_conv1)\n",
    "\n",
    "sensor_conv2 = Conv3D(CONV_NUM2, kernel_size=[1, 2, CONV_MERGE_LEN2],\n",
    "                strides=(1, 1,1), padding='SAME', activation=None, data_format='channels_last')(sensor_conv1)\n",
    "sensor_conv2 = BatchNormalization()(sensor_conv2)\n",
    "sensor_conv2 = Activation('relu')(sensor_conv2)\n",
    "sensor_conv2_shape = sensor_conv2.get_shape().as_list()\n",
    "sensor_conv2 = Dropout(CONV_KEEP_PROB,seed=None)(sensor_conv2)\n",
    "\n",
    "sensor_conv3 = Conv3D(CONV_NUM2, kernel_size=[1, 2, CONV_MERGE_LEN3],\n",
    "                strides=(1, 1,1), padding='SAME', activation=None, data_format='channels_last')(sensor_conv2)\n",
    "sensor_conv3 = BatchNormalization()(sensor_conv3)\n",
    "sensor_conv3 = Activation('relu')(sensor_conv3)\n",
    "sensor_conv3_shape = sensor_conv3.get_shape().as_list()\n",
    "print('sensor_conv3.shape ',sensor_conv3_shape)\n",
    "#sensor_conv_out = Reshape(([sensor_conv3_shape[1], sensor_conv3_shape[2]*sensor_conv3_shape[3]*sensor_conv3_shape[4]]))(sensor_conv3)\n",
    "#  tf.reshape(sensor_conv3, [sensor_conv3_shape[0], sensor_conv3_shape[1], sensor_conv3_shape[2]*sensor_conv3_shape[3]*sensor_conv3_shape[4]])\n",
    "print('sensor_conv_out.shape ',sensor_conv3.shape)\n",
    "# outputs = Flatten()(sensor_conv_out)\n",
    "\n",
    "outputs = Reshape((20,-1))(sensor_conv3)\n",
    "print('shape before gru ',outputs.shape)\n",
    "\n",
    "# outputs=  Reshape((20,8*64))(sensor_conv_in)\n",
    "\n",
    "\n",
    "outputs = GRU(INTER_DIM, activation='tanh',dropout=0.5,return_sequences=True)(outputs)\n",
    "#LSTM(INTER_DIM, return_sequences=True, kernel_initializer='orthogonal', name='LSTM_1')(sensor_conv_out)     #128\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "print('gru1.shape ',outputs.shape)\n",
    "\n",
    "def reduce_dim(x):\n",
    "    return x[:,-1:,:]\n",
    "\n",
    "# outputs = Lambda(reduce_dim)(outputs)\n",
    "\n",
    "# print('gru2_input',outputs[:,-4:,:].shape)\n",
    "outputs = GRU(INTER_DIM, activation='tanh',dropout=0.5,return_sequences=True)(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "print('gru2.shape ',outputs.shape)\n",
    "\n",
    "def reduce_timestep(x):\n",
    "    x = K.mean(x, axis=1, keepdims=False)\n",
    "    return x\n",
    "outputs = Lambda(reduce_timestep)(outputs)\n",
    "print('output of GRU ',outputs.shape)\n",
    "\n",
    "# outputs = Flatten()(outputs)\n",
    "\n",
    "outputs = Dense(6, activation='softmax',kernel_regularizer=regularizers.l2(5e-3))(outputs)\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.5, beta_2=0.9, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model = Model(inputs=[input1,input2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "      optimizer=adam,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "#model.fit([acc_inputs,gyro_inputs], train_y, batch_size=64, epochs=10)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 11:05:37.113312  4752 deprecation.py:323] From C:\\Users\\AYA04_Intern\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107172 samples, validate on 11908 samples\n",
      "Epoch 1/10\n",
      "107172/107172 [==============================] - 392s 4ms/step - loss: 0.5289 - acc: 0.7809 - val_loss: 0.3599 - val_acc: 0.8873\n",
      "Epoch 2/10\n",
      "107172/107172 [==============================] - 385s 4ms/step - loss: 0.2130 - acc: 0.9353 - val_loss: 0.3004 - val_acc: 0.9197\n",
      "Epoch 3/10\n",
      "107172/107172 [==============================] - 389s 4ms/step - loss: 0.1655 - acc: 0.9510 - val_loss: 0.2715 - val_acc: 0.9342\n",
      "Epoch 4/10\n",
      "107172/107172 [==============================] - 391s 4ms/step - loss: 0.1412 - acc: 0.9594 - val_loss: 0.2337 - val_acc: 0.9452\n",
      "Epoch 5/10\n",
      "107172/107172 [==============================] - 388s 4ms/step - loss: 0.1253 - acc: 0.9647 - val_loss: 0.2404 - val_acc: 0.9407\n",
      "Epoch 6/10\n",
      "107172/107172 [==============================] - 383s 4ms/step - loss: 0.1125 - acc: 0.9690 - val_loss: 0.2162 - val_acc: 0.9490\n",
      "Epoch 7/10\n",
      "107172/107172 [==============================] - 384s 4ms/step - loss: 0.1058 - acc: 0.9714 - val_loss: 0.1883 - val_acc: 0.9526\n",
      "Epoch 8/10\n",
      "107172/107172 [==============================] - 388s 4ms/step - loss: 0.0974 - acc: 0.9745 - val_loss: 0.1939 - val_acc: 0.9516\n",
      "Epoch 9/10\n",
      "107172/107172 [==============================] - 391s 4ms/step - loss: 0.0946 - acc: 0.9751 - val_loss: 0.2934 - val_acc: 0.9326\n",
      "Epoch 10/10\n",
      "107172/107172 [==============================] - 392s 4ms/step - loss: 0.0882 - acc: 0.9769 - val_loss: 0.2164 - val_acc: 0.9499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x78ec780>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.5, beta_2=0.9, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# model = Model(inputs=[input1,input2], outputs=outputs)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#       optimizer=adam,\n",
    "#       metrics=['accuracy'])\n",
    "# acc,gyo = np.split(x_train,2,axis=2)\n",
    "model.fit([acc,gyo], train_y, batch_size=64,validation_split=0.1, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    " \n",
    "model.save('DeepSense_keras.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    " \n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('DeepSense_keras_nomerge.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.5, beta_2=0.9, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#       optimizer=adam,\n",
    "#       metrics=['accuracy'])\n",
    "\n",
    "# model.fit([acc,gyo], train_y, batch_size=64,validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1193/1193 [==============================] - 1s 1ms/step\n",
      "loss, acc  0.33391250543802203 0.9161777031691456\n",
      "1193/1193 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       118\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.94      1.00      0.97       231\n",
      "           3       0.96      0.86      0.91       111\n",
      "           4       0.82      0.99      0.89       360\n",
      "           5       0.98      0.82      0.89       369\n",
      "\n",
      "   micro avg       0.90      0.92      0.91      1193\n",
      "   macro avg       0.77      0.77      0.77      1193\n",
      "weighted avg       0.91      0.92      0.91      1193\n",
      " samples avg       0.92      0.92      0.92      1193\n",
      "\n",
      "0.9116311006363353\n",
      "[[112   0   6   0   0   0]\n",
      " [  0   0   4   0   0   0]\n",
      " [  1   0 230   0   0   0]\n",
      " [  1   0   0  96  12   2]\n",
      " [  0   0   0   0 356   4]\n",
      " [  5   0   0   0  64 300]]\n"
     ]
    }
   ],
   "source": [
    "test_x = np.expand_dims(eval_x, axis=3)\n",
    "acc_test,gyo_test = np.split(test_x,2,axis=2)\n",
    "test_y = eval_y\n",
    "test_x = [acc_test,gyo_test]\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# print ( \"Loss = \" + str( preds[0] ) )\n",
    "# print ( \"Test Accuracy = \" + str( preds[1] ) )\n",
    "print(model.metrics_names)\n",
    "loss, acc = model.evaluate(test_x,test_y, batch_size=64)\n",
    "print('loss, acc ',loss,acc)\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_x, batch_size=128, verbose=1)\n",
    "# y_pred_bool = np.argmax(preds, axis=1)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0\n",
    "\n",
    "print(classification_report(test_y, y_pred))\n",
    "print(metrics.f1_score(test_y, y_pred, average=\"weighted\"))\n",
    "print(confusion_matrix(test_y.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = deepSense(train_x, True, name='deepSense')\n",
    "\n",
    "# predict = tf.argmax(logits, axis=1)\n",
    "\n",
    "# batchLoss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=train_y)\n",
    "# loss = tf.reduce_mean(batchLoss)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "# \ttf.global_variables_initializer().run()\n",
    "# \t# coord = tf.train.Coordinator()\n",
    "# \t# threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "# \tfor iteration in range(TOTAL_ITER_NUM):\n",
    "\n",
    "# \t\t# _, lossV, _trainY, _predict = sess.run([discOptimizer, loss, trainY, predict], feed_dict = {\n",
    "# \t\t# \ttrain_status: True\n",
    "# \t\t# \t})\n",
    "# \t\t_, lossV, _trainY, _predict = sess.run([discOptimizer, loss, train_y, predict])\n",
    "# \t\t_label = np.argmax(_trainY, axis=1)\n",
    "# \t\t_accuracy = np.mean(_label == _predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category  0 {0.0: 105040, 1.0: 14040}\n",
      "category  1 {0.0: 108870, 1.0: 10210}\n",
      "category  2 {0.0: 106900, 1.0: 12180}\n",
      "category  3 {0.0: 111250, 1.0: 7830}\n",
      "category  4 {0.0: 80310, 1.0: 38770}\n",
      "category  5 {0.0: 83030, 1.0: 36050}\n",
      "\n",
      "category  0 {0.0: 1075, 1.0: 118}\n",
      "category  1 {0.0: 1189, 1.0: 4}\n",
      "category  2 {0.0: 962, 1.0: 231}\n",
      "category  3 {0.0: 1082, 1.0: 111}\n",
      "category  4 {0.0: 833, 1.0: 360}\n",
      "category  5 {0.0: 824, 1.0: 369}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(train_y)\n",
    "for ele in range(6):\n",
    "    print('category ',ele,  dict(df[ele].value_counts()))\n",
    "df1 = pd.DataFrame(eval_y)\n",
    "print()\n",
    "for ele in range(6):\n",
    "    print('category ',ele,  dict(df1[ele].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
